---
title: "Lab 10 - Grading the professor, Pt. 1"
author: "Jamieson Nathan"
date: "4/20/2025"
output: github_document
---

## Load Packages and Data

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse) 
library(broom)
library(openintro)

data(evals)
?evals  
glimpse(evals)  
```

## Exercise 1

```{r first-one}

ggplot(evals, aes(x = score)) +
  geom_histogram(binwidth = 0.1, fill = "steelblue", color = "white") +
  labs(title = "Distribution of Course Evaluation Scores", x = "Score", y = "Count")

summary(evals$score)

ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Evaluation Score vs. Beauty Rating",
       x = "Average Beauty Rating",
       y = "Evaluation Score")

ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter(width = 0.2, height = 0.1) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Jittered Plot: Score vs. Beauty Rating",
       x = "Average Beauty Rating",
       y = "Evaluation Score")


```

The histogram is left-skewed showing that typically students give high ratings (4-5). This is reaffirmed by the summary output which shows a means of almost 4.2. 

The scatter plot indicates some sort of upward linear trend, which may suggest that more attractive teachers get slightly higher scores, however, there is a lot of noise and I would be hesitant to conclude that definitely. 

The jittered plot helps visualize the density a lot more, and helps give a more accurate impression of frequencies. 

## Exercise 2

```{r second-one}

m_bty <- lm(score ~ bty_avg, data = evals)
summary(m_bty)


```
So the linera model is **y^ =3.88+0.067⋅bty_avg**

```{r third-one}

ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter(width = 0.2, height = 0.1) +
  geom_smooth(method = "lm", se = FALSE, color = "orange") +
  labs(title = "Regression of Evaluation Score on Beauty",
       x = "Average Beauty Rating",
       y = "Evaluation Score")

summary(m_bty)$r.squared


```

The shaded area is the confidence interval around the regression line. You are asking us to turn it off to focus only on the trend, without being distracted by the uncertainty visualization.

Since The slope is about 0.06, for every 1-point increase in average beauty rating, a professor's evaluation score increases by 0.067 points, on average.So, yes, beauty does have a small but positive effect on evaluation scores.

The intercept is 3.88, which is the predicted evaluation score for a professor with a beauty rating of 0. Since beauty ratings don’t actually go that low in this data set, the intercept is a mathematical artifact here. 

Looking at the R^2 value, only about 3.5% of the variation in evaluation scores is explained by beauty ratings.So beauty does have a measurable effect, but it’s not a strong predictor and most of the variation in evaluation scores is due to other factors.

## Exercise 3

```{r fourth-one}

m_gen <- lm(score ~ gender, data = evals)
tidy(m_gen)

```

The intercept always corresponds to the reference category of the categorical variable—in this case. On average, male professors score 0.142 points higher than female professors in student evaluations. The p-value (0.00558) shows this difference is statistically significant at the 0.01 level.

Line equations: y^males = 4.09 + 0.142 = 4.232, y^females = 4.09 


```{r fifth-one}

m_rank <- lm(score ~ rank, data = evals)
tidy(m_rank)

```
Tenure track professors are rated 0.13 points lower and tenured professors 0.15 points lower than non-tenure track professors (intercept = 4.28), suggesting a small but significant decrease in evaluations with seniority.

## Exercise 1

```{r sixth-one}

evals <- evals %>%
  mutate(rank_relevel = relevel(rank, ref = "tenured"))

m_rank_relevel <- lm(score ~ rank_relevel, data = evals)
summary(m_rank_relevel)

summary(m_rank_relevel)$r.squared

```
Using relevel sets tenured professors as the reference group, so the model compares other ranks against tenured. In this model, tenured professors average a score of 4.139; non-tenure track professors score 0.145 points higher, and tenure track professors score just 0.016 points higher. Rank explains only 1.13% of the variation in evaluation scores, indicating it’s not a strong predictor.

```{r seventh-one}

evals <- evals %>%
  mutate(tenure_eligible = ifelse(rank == "teaching", "no", "yes")) %>%
  mutate(tenure_eligible = factor(tenure_eligible))

m_tenure_eligible <- lm(score ~ tenure_eligible, data = evals)
summary(m_tenure_eligible)

summary(m_tenure_eligible)$r.squared

```

Tenure-eligible professors score 0.112 points lower on average than non-tenure faculty (who average 4.284), but tenure eligibility explains only 0.66% of the variation in evaluation scores, making it a weak predictor.
